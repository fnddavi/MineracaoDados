import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# CARREGAR E PREPARAR OS DADOS
# ============================================================================

# Carregar dataset
# Certifique-se de que o arquivo 'breast-cancer.csv' est√° na mesma pasta
try:
    df = pd.read_csv('breast-cancer.csv')
except FileNotFoundError:
    print("Erro: Arquivo 'breast-cancer.csv' n√£o encontrado. Verifique se ele est√° na pasta atual.")
    exit()

# Substituir '?' por NaN
df_clean = df.copy()
for col in df_clean.columns:
    if df_clean[col].dtype == 'object':
        df_clean[col] = df_clean[col].replace('?', np.nan)

print("="*80)
print("ü§ñ QUEST√ÉO 6 - CLASSIFICA√á√ÉO (Seguindo EXATAMENTE as instru√ß√µes)")
print("="*80)

# Separar features e target
X = df_clean.drop('Class', axis=1)
y = df_clean['Class']

# Codificar a vari√°vel target
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

print(f"üìä Dimens√µes:")
print(f"   ‚Ä¢ X (features): {X.shape}")
print(f"   ‚Ä¢ y (target): {y.shape}")
print(f"   ‚Ä¢ Classes: {np.unique(y, return_counts=True)}")

# ============================================================================
# M√âTODO 1: EXATAMENTE como a professora exemplificou
# ============================================================================

print("\n" + "="*80)
print("üìù M√âTODO 1: Aplicando OneHotEncoder EXATAMENTE como no exemplo da professora")
print("="*80)

# Identificar colunas categ√≥ricas (todas exceto 'deg-malig' que √© num√©rica)
categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
print(f"üìã Colunas categ√≥ricas (que ser√£o convertidas para bin√°rio):")
for col in categorical_cols:
    unique_vals = X[col].dropna().unique()[:5]  # Mostrar primeiros 5 valores √∫nicos
    print(f"   ‚Ä¢ {col}: {len(X[col].dropna().unique())} valores √∫nicos, ex: {list(unique_vals)}")

# PASSO 1: Substituir valores ausentes pela MODA
print("\nüîß PASSO 1: Substituindo valores ausentes pela MODA...")
for col in X.columns:
    if X[col].isnull().any():
        moda = X[col].mode()[0] if not X[col].mode().empty else 'unknown'
        X[col] = X[col].fillna(moda)
        print(f"   ‚Ä¢ {col}: substitu√≠dos {X[col].isnull().sum()} valores pela moda '{moda}'")

# Verificar se ainda h√° valores ausentes
print(f"\n‚úÖ Ap√≥s imputa√ß√£o pela moda: {X.isnull().sum().sum()} valores ausentes restantes")

# PASSO 2: Aplicar OneHotEncoder EXATAMENTE como no exemplo da professora
print("\nüîß PASSO 2: Aplicando OneHotEncoder...")
print("   encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=True)")

# Criar o encoder exatamente como no exemplo
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=True)

# Aplicar transforma√ß√£o (apenas nas colunas categ√≥ricas)
# Primeiro, vamos codificar apenas as colunas categ√≥ricas
X_categorical = X[categorical_cols]

# Fit e transform
print("   X_encoded = encoder.fit_transform(X_categorical).toarray()")
X_encoded = encoder.fit_transform(X_categorical).toarray()

print(f"\nüìä Resultado da codifica√ß√£o:")
print(f"   ‚Ä¢ Forma original de X_categorical: {X_categorical.shape}")
print(f"   ‚Ä¢ Forma ap√≥s OneHotEncoder: {X_encoded.shape}")
print(f"   ‚Ä¢ N√∫mero de features bin√°rias criadas: {X_encoded.shape[1]}")

# Agora, precisamos combinar com as colunas num√©ricas (deg-malig)
numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
print(f"\nüî¢ Colunas num√©ricas (n√£o convertidas para bin√°rio): {numeric_cols}")

if numeric_cols:
    X_numeric = X[numeric_cols].values
    # Concatenar features num√©ricas com features codificadas
    X_final = np.hstack([X_numeric, X_encoded])
    print(f"   ‚Ä¢ Forma final de X (num√©ricas + categ√≥ricas codificadas): {X_final.shape}")
else:
    X_final = X_encoded

# ============================================================================
# CLASSIFICA√á√ÉO COM CROSS-VALIDATION (10 FOLDS)
# ============================================================================

print("\n" + "="*80)
print("üéØ CLASSIFICA√á√ÉO COM CROSS-VALIDATION (10 FOLDS)")
print("="*80)

# Definir os classificadores
j48 = DecisionTreeClassifier(random_state=42, criterion='entropy')
naive_bayes = GaussianNB()

# Configurar cross-validation (10 folds)
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

print(f"\nüîß Configura√ß√£o da valida√ß√£o cruzada:")
print(f"   ‚Ä¢ N√∫mero de folds: 10")
print(f"   ‚Ä¢ Total de inst√¢ncias: {len(X_final)}")
print(f"   ‚Ä¢ Total de features: {X_final.shape[1]}")

# Avaliar J48
print("\nüìä Avaliando J48 (√Årvore de Decis√£o)...")
j48_scores = cross_val_score(j48, X_final, y_encoded, cv=cv, scoring='accuracy')
j48_mean = j48_scores.mean()
j48_std = j48_scores.std()

print(f"   ‚Ä¢ Scores por fold: {j48_scores}")
print(f"   ‚Ä¢ Acur√°cia m√©dia: {j48_mean:.4f} ({j48_mean*100:.2f}%)")
print(f"   ‚Ä¢ Desvio padr√£o: {j48_std:.4f}")

# Avaliar Naive Bayes
print("\nüìä Avaliando Naive Bayes...")
nb_scores = cross_val_score(naive_bayes, X_final, y_encoded, cv=cv, scoring='accuracy')
nb_mean = nb_scores.mean()
nb_std = nb_scores.std()

print(f"   ‚Ä¢ Scores por fold: {nb_scores}")
print(f"   ‚Ä¢ Acur√°cia m√©dia: {nb_mean:.4f} ({nb_mean*100:.2f}%)")
print(f"   ‚Ä¢ Desvio padr√£o: {nb_std:.4f}")

# ============================================================================
# RESULTADO FINAL
# ============================================================================

print("\n" + "="*80)
print("üèÜ RESULTADO FINAL - QUEST√ÉO 6")
print("="*80)

print(f"\nüìà DESEMPENHO DOS ALGORITMOS:")
print("-" * 50)

# Criar DataFrame para visualiza√ß√£o
results_df = pd.DataFrame({
    'Algoritmo': ['J48 (√Årvore de Decis√£o)', 'Naive Bayes'],
    'Acur√°cia M√©dia': [j48_mean, nb_mean],
    'Desvio Padr√£o': [j48_std, nb_std],
    'Acur√°cia (%)': [f"{j48_mean*100:.2f}%", f"{nb_mean*100:.2f}%"]
})

print(results_df.to_string(index=False))

# Determinar o melhor algoritmo
print("\n" + "="*50)
print("üéñÔ∏è  MELHOR ALGORITMO:")
print("="*50)

if j48_mean > nb_mean:
    print(f"‚úÖ J48 (√Årvore de Decis√£o) apresentou o MELHOR resultado!")
    print(f"   ‚Ä¢ Acur√°cia: {j48_mean*100:.2f}%")
    print(f"   ‚Ä¢ Superioridade: {abs(j48_mean - nb_mean)*100:.2f}% sobre Naive Bayes")
elif nb_mean > j48_mean:
    print(f"‚úÖ Naive Bayes apresentou o MELHOR resultado!")
    print(f"   ‚Ä¢ Acur√°cia: {nb_mean*100:.2f}%")
    print(f"   ‚Ä¢ Superioridade: {abs(nb_mean - j48_mean)*100:.2f}% sobre J48")
else:
    print("‚ö†Ô∏è  Os dois algoritmos apresentaram resultados equivalentes!")

# Visualiza√ß√£o comparativa (Salvar em arquivo em vez de mostrar na tela)
plt.figure(figsize=(12, 6))

# Gr√°fico 1: Acur√°cia m√©dia
plt.subplot(1, 2, 1)
algorithms = ['J48', 'Naive Bayes']
accuracies = [j48_mean, nb_mean]
colors = ['skyblue', 'lightcoral']

bars = plt.bar(algorithms, accuracies, color=colors, alpha=0.8, edgecolor='black')
plt.title('Acur√°cia M√©dia dos Algoritmos', fontsize=14, fontweight='bold')
plt.ylabel('Acur√°cia', fontsize=12)
plt.ylim([0, 1.0])
plt.grid(axis='y', alpha=0.3)

# Adicionar valores nas barras
for bar, acc in zip(bars, accuracies):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{acc:.3f}\n({acc*100:.1f}%)',
             ha='center', va='bottom', fontsize=10)

# Gr√°fico 2: Scores por fold
plt.subplot(1, 2, 2)
folds = range(1, 11)
plt.plot(folds, j48_scores, 'o-', color='skyblue', linewidth=2, markersize=8, label='J48')
plt.plot(folds, nb_scores, 's-', color='lightcoral', linewidth=2, markersize=8, label='Naive Bayes')
plt.title('Scores por Fold (10-Fold CV)', fontsize=14, fontweight='bold')
plt.xlabel('Fold', fontsize=12)
plt.ylabel('Acur√°cia', fontsize=12)
plt.xticks(folds)
plt.ylim([0, 1.0])
plt.grid(True, alpha=0.3)
plt.legend()

plt.suptitle('Compara√ß√£o de Algoritmos de Classifica√ß√£o', fontsize=16, fontweight='bold')
plt.tight_layout()

# Salvar o gr√°fico
plt.savefig('resultado_comparacao.png')
print("\nüìä Gr√°fico salvo como 'resultado_comparacao.png'")
